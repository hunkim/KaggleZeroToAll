{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# West Nile Virus Prediction\n",
    "https://www.kaggle.com/c/predict-west-nile-virus\n",
    "\n",
    "base code : https://www.kaggle.com/duttaroy/enhanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem/data description\n",
    "\n",
    "In this competition, you will be analyzing weather data and GIS data and predicting whether or not West Nile virus is present, for a given time, location, and species. \n",
    "\n",
    "Every year from late-May to early-October, public health workers in Chicago setup mosquito traps scattered across the city. Every week from Monday through Wednesday, these traps collect mosquitos, and the mosquitos are tested for the presence of West Nile virus before the end of the week. The test results include the number of mosquitos, the mosquitos species, and whether or not West Nile virus is present in the cohort. \n",
    "\n",
    "## train.csv, test.csv \n",
    "The training set consists of data from 2007, 2009, 2011, and 2013, while in the test set you are requested to predict the test results for 2008, 2010, 2012, and 2014.\n",
    "\n",
    "* Id: the id of the record\n",
    "* Date: date that the WNV test is performed\n",
    "* Address: approximate address of the location of trap. This is used to send to the GeoCoder. \n",
    "* Species: the species of mosquitos\n",
    "* Block: block number of address\n",
    "* Street: street name\n",
    "* Trap: Id of the trap\n",
    "* AddressNumberAndStreet: approximate address returned from GeoCoder\n",
    "* Latitude, Longitude: Latitude and Longitude returned from GeoCoder\n",
    "* AddressAccuracy: accuracy returned from GeoCoder\n",
    "* NumMosquitos: number of mosquitoes caught in this trap\n",
    "* WnvPresent: whether West Nile Virus was present in these mosquitos. 1 means WNV is present, and 0 means not present. \n",
    "\n",
    "## spray.csv\n",
    "GIS data of spraying efforts in 2011 and 2013\n",
    "* Date, Time: the date and time of the spray\n",
    "* Latitude, Longitude: the Latitude and Longitude of the spray\n",
    "\n",
    "## weather.csv \n",
    "weather data from 2007 to 2014. Column descriptions in noaa_weather_qclcd_documentation.pdf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading data\n",
    "Show how to download and load them in Kerans, TensorFlow, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data convert function and fill missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date(text):\n",
    "    return datetime.datetime.strptime(text, \"%Y-%m-%d\").date()\n",
    "\n",
    "def ll(text):\n",
    "     return int(float(text)*100)/100\n",
    "\n",
    "def precip(text):\n",
    "    TRACE = 1e-3\n",
    "    text = text.strip()\n",
    "    if text == \"M\":\n",
    "        return None\n",
    "    if text == \"-\":\n",
    "        return None\n",
    "    if text == \"T\":\n",
    "        return TRACE\n",
    "    return float(text)\n",
    "\n",
    "def impute_missing_weather_station_values(weather):\n",
    "    for k, v in weather.items():\n",
    "        if v[0] is None:\n",
    "            v[0] = v[1]\n",
    "        elif v[1] is None:\n",
    "            v[1] = v[0]\n",
    "        for k1 in v[0]:\n",
    "            if v[0][k1] is None:\n",
    "                v[0][k1] = v[1][k1]\n",
    "        for k1 in v[1]:\n",
    "            if v[1][k1] is None:\n",
    "                v[1][k1] = v[0][k1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load function\n",
    "\n",
    "```python\n",
    "for line in csv.DictReader(open(\"dataPath\")): # open csvfile and access line by line\n",
    "    for name, converter in feature_dict: # feature_dict contain col name and data converter to use\n",
    "        line[name] = converter(line[name]) # convert data\n",
    "   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_weather():\n",
    "    weather = {}\n",
    "    for line in csv.DictReader(open(\"input/weather.csv\")):\n",
    "        for name, converter in {\"Date\" : date,\n",
    "                                \"Tmax\" : float,\"Tmin\" : float,\"Tavg\" : float,\n",
    "                                \"DewPoint\" : float, \"WetBulb\" : float,\n",
    "                                \"PrecipTotal\" : precip,\"Sunrise\" : precip,\"Sunset\" : precip,\n",
    "                                \"Depart\" : float, \"Heat\" : precip,\"Cool\" : precip,\n",
    "                                \"ResultSpeed\" : float,\"ResultDir\" : float,\"AvgSpeed\" : float,\n",
    "                                \"StnPressure\" : float, \"SeaLevel\" : float}.items():\n",
    "            \n",
    "            x = line[name].strip()\n",
    "            line[name] = converter(x) if (x != \"M\") else None\n",
    "            \n",
    "        station = int(line[\"Station\"]) - 1\n",
    "        \n",
    "        dt = line[\"Date\"]\n",
    "        if dt not in weather:\n",
    "            weather[dt] = [None, None]\n",
    "            \n",
    "        weather[dt][station] = line\n",
    "    impute_missing_weather_station_values(weather) # fill missing values       \n",
    "    return weather\n",
    "    \n",
    "def load_train():\n",
    "    training = []\n",
    "    for line in csv.DictReader(open(\"input/train.csv\")):\n",
    "        for name, converter in {\"Date\" : date, \n",
    "                                \"Latitude\" : ll, \"Longitude\" : ll,\n",
    "                                \"NumMosquitos\" : int, \"WnvPresent\" : int}.items():\n",
    "            line[name] = converter(line[name])\n",
    "        training.append(line)\n",
    "    return training\n",
    "    \n",
    "def load_test():\n",
    "    training = []\n",
    "    for line in csv.DictReader(open(\"input/test.csv\")):\n",
    "        for name, converter in {\"Date\" : date, \n",
    "                                \"Latitude\" : ll, \"Longitude\" : ll}.items():\n",
    "            line[name] = converter(line[name])\n",
    "        training.append(line)\n",
    "    return training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(X, mean=None, std=None):\n",
    "    count = X.shape[1]\n",
    "    if mean is None:\n",
    "        mean = np.nanmean(X, axis=0)\n",
    "    for i in range(count):\n",
    "        X[np.isnan(X[:,i]), i] = mean[i]\n",
    "    if std is None:\n",
    "        std = np.std(X, axis=0)\n",
    "    for i in range(count):\n",
    "        X[:,i] = (X[:,i] - mean[i]) / std[i]\n",
    "    return mean, std\n",
    "    \n",
    "def scaled_count(record):\n",
    "    SCALE = 9.0\n",
    "    if \"NumMosquitos\" not in record:\n",
    "        # This is test data\n",
    "        return 1\n",
    "    return int(np.ceil(record[\"NumMosquitos\"] / SCALE))\n",
    "\n",
    "def get_closest_station(lat, long):\n",
    "    # Chicago is small enough that we can treat coordinates as rectangular.\n",
    "    stations = np.array([[41.995, -87.933],\n",
    "                         [41.786, -87.752]])\n",
    "    loc = np.array([lat, long])\n",
    "    deltas = stations - loc[None, :]\n",
    "    dist2 = (deltas**2).sum(1)\n",
    "    return np.argmin(dist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "species_map = {'CULEX RESTUANS' : \"100000\",\n",
    "              'CULEX TERRITANS' : \"010000\", \n",
    "              'CULEX PIPIENS'   : \"001000\", \n",
    "              'CULEX PIPIENS/RESTUANS' : \"101000\", \n",
    "              'CULEX ERRATICUS' : \"000100\", \n",
    "              'CULEX SALINARIUS': \"000010\", \n",
    "              'CULEX TARSALIS' :  \"000001\",\n",
    "              'UNSPECIFIED CULEX': \"001100\"} # hack! https://www.kaggle.com/c/predict-west-nile-virus/discussion/13810\n",
    "\n",
    "# species name: vector\n",
    "    \n",
    "        \n",
    "def assemble_X(base, weather):\n",
    "    X = []\n",
    "    for b in base:\n",
    "        date = b[\"Date\"]\n",
    "        date2 = np.sin((2 * math.pi * date.day) / 365 * 24)\n",
    "        date3 = np.cos((2 * math.pi * date.day) / 365 * 24)\n",
    "        date4 = np.sin((2 * math.pi * date.month) / 365)\n",
    "        \n",
    "        lat, longi = b[\"Latitude\"], b[\"Longitude\"]\n",
    "        case = [date.year, date.month, date4, date.day, date.weekday(), date2, date3, lat, longi]\n",
    "        \n",
    "        # Look at a selection of past weather values\n",
    "        for days_ago in [1,2,3,5,8,12]:\n",
    "            day = date - datetime.timedelta(days=days_ago)\n",
    "            for obs in [\"Tmax\", \"Tmin\", \"Tavg\", \"DewPoint\", \"WetBulb\",\n",
    "                        \"PrecipTotal\", \"Depart\", \"Sunrise\", \"Sunset\",\n",
    "                        \"Heat\", \"Cool\", \"ResultSpeed\", \"ResultDir\"]:\n",
    "                \n",
    "                station = get_closest_station(lat, longi)\n",
    "                case.append(weather[day][station][obs])\n",
    "                \n",
    "        # Specify which mosquitos are present\n",
    "        species_vector = [float(x) for x in species_map[b[\"Species\"]]]\n",
    "        case.extend(species_vector)\n",
    "        \n",
    "        '''\n",
    "        Weight each observation by the number of mosquitos seen.\n",
    "        Test data Doesn't have this column, so in that case use 1. \n",
    "        This accidentally Takes into account multiple entries that \n",
    "        result from >50 mosquitos on one day.\n",
    "        '''\n",
    "        \n",
    "        for repeat in range(scaled_count(b)):\n",
    "            X.append(case)    \n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    return X\n",
    "    \n",
    "def assemble_y(base):\n",
    "    y = []\n",
    "    for b in base:\n",
    "        present = b[\"WnvPresent\"]\n",
    "        for repeat in range(scaled_count(b)):\n",
    "            y.append(present)    \n",
    "    return np.asarray(y, dtype=np.float32).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cache_data(data, path):\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "    else:\n",
    "        print('Directory doesnt exists')\n",
    "\n",
    "def restore_data(path):\n",
    "    data = dict()\n",
    "    if os.path.isfile(path):\n",
    "        file = open(path, 'rb')\n",
    "        data = pickle.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use only once for data dump\n",
    "\n",
    "train_data = load_train() \n",
    "test_data = load_test()\n",
    "weather_data = load_weather()\n",
    "\n",
    "x_train = assemble_X(train_data, weather_data)\n",
    "x_test = assemble_X(test_data, weather_data) \n",
    "\n",
    "y_train = assemble_y(train_data)\n",
    "\n",
    "mean, std = normalize(x_train)\n",
    "mean, std = normalize(x_test, mean, std)\n",
    "\n",
    "cache_data(test_data, './test_data.dump')\n",
    "\n",
    "cache_data(x_train, './x_train.dump')\n",
    "cache_data(x_test, './x_test.dump')\n",
    "cache_data(y_train, './y_train.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = restore_data('./test_data.dump') # for submission\n",
    "\n",
    "x_train = restore_data('./x_train.dump')\n",
    "x_test = restore_data('./x_test.dump')\n",
    "y_train = restore_data('./y_train.dump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "Model implementation. It can be divided to several small sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21260, 93)\n"
     ]
    }
   ],
   "source": [
    "input_size = x_train.shape[1]\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, input_size], name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1], name=\"Y\")\n",
    "dropout = tf.placeholder(tf.float32, shape=None)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([input_size, 100], name='w1'))\n",
    "W2 = tf.Variable(tf.random_normal([100, 100], name='w2'))\n",
    "W3 = tf.Variable(tf.random_normal([100, 1], name='w3'))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([100], name='b1'))\n",
    "b2 = tf.Variable(tf.random_normal([100], name='b2'))\n",
    "b3 = tf.Variable(tf.random_normal([1], name='b3'))\n",
    "\n",
    "\n",
    "layer1 = tf.matmul(X, W1) + b1\n",
    "layer1 = tf.nn.dropout(layer1, dropout)\n",
    "layer2 = tf.matmul(layer1, W2) + b2\n",
    "layer2 = tf.nn.dropout(layer2, dropout)\n",
    "layer3 = tf.matmul(layer2, W3) + b3\n",
    "\n",
    "pred = tf.nn.sigmoid(layer3)\n",
    "                 \n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=layer3, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 cost: 768.28618\n",
      "epoch: 100 cost: 0.66764\n",
      "epoch: 200 cost: 0.47354\n",
      "epoch: 300 cost: 0.25337\n",
      "epoch: 400 cost: 0.25529\n",
      "epoch: 500 cost: 0.26151\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epochs = 501\n",
    "batch_size = 500\n",
    "\n",
    "for e in range(epochs):\n",
    "    avg_cost = 0 \n",
    "\n",
    "    for step in range(0, len(x_train), batch_size):\n",
    "        batch_mask = np.random.choice(len(x_train), batch_size) # For dataset shuffle\n",
    "        \n",
    "        feed_dict = {X: x_train[batch_mask], \n",
    "                     Y: y_train[batch_mask], dropout: 0.3}\n",
    "        \n",
    "        _, c = sess.run([optimizer, cost], feed_dict=feed_dict)\n",
    "        \n",
    "        avg_cost += c\n",
    "        \n",
    "    \n",
    "    if e % 100 == 0:\n",
    "        avg_cost = avg_cost / (len(x_train) / batch_size) \n",
    "        print(\"epoch: {} cost: {:.5f}\".format(e, avg_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = sess.run(pred, feed_dict={X: x_test, dropout: 1})[:, 0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"west_nile.csv\", \"w\") as csv_file:\n",
    "    out = csv.writer(csv_file)\n",
    "    out.writerow([\"Id\",\"WnvPresent\"])\n",
    "    for row, p in zip(test_data, predictions):\n",
    "        out.writerow([row[\"Id\"], p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future work/exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ensemble, Parameter Tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
