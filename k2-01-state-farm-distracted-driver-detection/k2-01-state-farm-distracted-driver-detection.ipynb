{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Farm Distracted Driver Detection\n",
    "https://www.kaggle.com/c/state-farm-distracted-driver-detection\n",
    "\n",
    "## Reference\n",
    "https://www.kaggle.com/zfturbo/keras-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem/data description\n",
    "When you pass the offending driver, what do you expect to see? You certainly aren't surprised when you spot a driver who is texting, seemingly enraptured by social media, or in a lively hand-held conversation on their phone.\n",
    "\n",
    "According to the CDC motor vehicle safety division, one in five car accidents is caused by a distracted driver. Sadly, this translates to 425,000 people injured and 3,000 people killed by distracted driving every year.\n",
    "\n",
    "State Farm hopes to improve these alarming statistics, and better insure their customers, by testing whether dashboard cameras can automatically detect drivers engaging in distracted behaviors. Given a dataset of 2D dashboard camera images, State Farm is challenging Kagglers to classify each driver's behavior. Are they driving attentively, wearing their seatbelt, or taking a selfie with their friends in the backseat?  \n",
    "\n",
    "![](https://kaggle2.blob.core.windows.net/competitions/kaggle/5048/media/output_DEb8oT.gif)\n",
    "  \n",
    "  \n",
    "The 10 classes to predict are:  \n",
    "\n",
    "c0: safe driving  \n",
    "c1: texting - right  \n",
    "c2: talking on the phone - right  \n",
    "c3: texting - left  \n",
    "c4: talking on the phone - left  \n",
    "c5: operating the radio  \n",
    "c6: drinking  \n",
    "c7: reaching behind  \n",
    "c8: hair and makeup  \n",
    "c9: talking to passenger  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading data\n",
    "First execute data_download.sh to download CSV/img files:\n",
    "\n",
    "```bash\n",
    "$ bash data_download.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_im_cv2_mod(path, img_rows, img_cols, color_type=1):\n",
    "    # Load as grayscale\n",
    "    if color_type == 1:\n",
    "        img = cv2.imread(path, 0)\n",
    "    else:\n",
    "        img = cv2.imread(path)\n",
    "        \n",
    "    # Image Rotation: make CNN Architecture cover rotating images.  \n",
    "    rotate = random.uniform(-10, 10)\n",
    "    M = cv2.getRotationMatrix2D((img.shape[1]/2, img.shape[0]/2), rotate, 1)\n",
    "    img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    # Reduce size for reduce GPU memory usage and computation.\n",
    "    resized = cv2.resize(img, (img_cols, img_rows), cv2.INTER_LINEAR)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train(img_rows, img_cols, color_type=1):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('Read train images')\n",
    "    for j in range(10):\n",
    "        print('Load folder c{}'.format(j))\n",
    "        path = os.path.join('train', 'c' + str(j), '*.jpg')\n",
    "        files = glob.glob(path) # Returns a list of all files and directories corresponding to the path.\n",
    "        for fl in files:\n",
    "            flbase = os.path.basename(fl)\n",
    "            img = get_im_cv2_mod(fl, img_rows, img_cols, color_type)\n",
    "            X_train.append(img/255)\n",
    "            y_train.append(j)\n",
    "            \n",
    "    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test(img_rows, img_cols, color_type=1):\n",
    "    print('Read test images')\n",
    "    start_time = time.time()\n",
    "    path = os.path.join('test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    thr = math.floor(len(files)/10)\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im_cv2_mod(fl, img_rows, img_cols, color_type)\n",
    "        X_test.append(img/255)\n",
    "        X_test_id.append(flbase)\n",
    "        total += 1\n",
    "        if total%thr == 0:\n",
    "            print('Read {} images from {}'.format(total, len(files)))\n",
    "    \n",
    "    print('Read test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    \n",
    "    return X_test, X_test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_width = 64\n",
    "img_height = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read drivers data\n",
      "Read train images\n",
      "Load folder c0\n",
      "Load folder c1\n",
      "Load folder c2\n",
      "Load folder c3\n",
      "Load folder c4\n",
      "Load folder c5\n",
      "Load folder c6\n",
      "Load folder c7\n",
      "Load folder c8\n",
      "Load folder c9\n",
      "Read train data time: 55.31 seconds\n",
      "(22424, 64, 64, 1) (22424,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = load_train(img_width, img_height)\n",
    "x_train = np.expand_dims(np.array(x_train, dtype=np.float32), axis=-1) \n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read test images\n",
      "Read 7972 images from 79726\n",
      "Read 15944 images from 79726\n",
      "Read 23916 images from 79726\n",
      "Read 31888 images from 79726\n",
      "Read 39860 images from 79726\n",
      "Read 47832 images from 79726\n",
      "Read 55804 images from 79726\n",
      "Read 63776 images from 79726\n",
      "Read 71748 images from 79726\n",
      "Read 79720 images from 79726\n",
      "Read test data time: 172.47 seconds\n",
      "(79726, 64, 64, 1) (79726,)\n"
     ]
    }
   ],
   "source": [
    "x_test, test_id = load_test(img_width, img_height)\n",
    "x_test = np.expand_dims(np.array(x_test, dtype=np.float32), axis=-1) \n",
    "test_id = np.array(test_id)\n",
    "\n",
    "print(x_test.shape, test_id.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "Model implementation. It can be divided to several small sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 10\n",
    "batch_size = 32 # it is max batch size fit aws p2 instance GPU memory: 12G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(np.float32, shape=[None, img_width, img_height, 1])\n",
    "Y = tf.placeholder(np.float32, shape=[None])\n",
    "dropout = tf.placeholder(np.float32)\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=X, filters=32, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "pool1_dp = tf.nn.dropout(pool1, dropout)\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=pool1_dp, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "pool2_dp = tf.nn.dropout(pool2, dropout)\n",
    "\n",
    "conv3 = tf.layers.conv2d(inputs=pool2_dp, filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "pool3_dp = tf.nn.dropout(pool3, dropout)\n",
    "\n",
    "pool3_flat = tf.reshape(pool3_dp, [-1, 8 * 8 * 128]) # w * h * d for pool3\n",
    "\n",
    "fc1 = tf.layers.dense(inputs=pool3_flat, units=1024, activation=tf.nn.relu)\n",
    "dropout1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "fc2 = tf.layers.dense(inputs=dropout1, units=512, activation=tf.nn.relu)\n",
    "dropout2 = tf.nn.dropout(fc2, dropout)\n",
    "\n",
    "output = tf.layers.dense(inputs=dropout2, units=10)\n",
    "pred = tf.nn.softmax(output)\n",
    "\n",
    "onehot = tf.one_hot(indices=tf.cast(Y, tf.int32), depth=10)\n",
    "cost = tf.losses.softmax_cross_entropy(onehot_labels=onehot, logits=output)\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(output, axis=1), tf.argmax(onehot, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 cost: 2.25432 acc: 0.15796\n",
      "epoch: 1 cost: 1.83217 acc: 0.31841\n",
      "epoch: 2 cost: 1.48811 acc: 0.44863\n",
      "epoch: 3 cost: 1.21254 acc: 0.56373\n",
      "epoch: 4 cost: 0.99937 acc: 0.64088\n",
      "epoch: 5 cost: 0.81758 acc: 0.71544\n",
      "epoch: 6 cost: 0.68962 acc: 0.76347\n",
      "epoch: 7 cost: 0.57965 acc: 0.80374\n",
      "epoch: 8 cost: 0.49041 acc: 0.83625\n",
      "epoch: 9 cost: 0.44493 acc: 0.85310\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    \n",
    "    avg_cost = 0 \n",
    "    avg_acc = 0\n",
    "    \n",
    "    for step in range(0, len(x_train), batch_size):\n",
    "        batch_mask = np.random.choice(len(x_train), batch_size) # For dataset shuffle\n",
    "        \n",
    "        feed_dict = {X: x_train[batch_mask], \n",
    "                     Y: y_train[batch_mask], dropout: 0.5}\n",
    "        \n",
    "        _, c, a = sess.run([train, cost, accuracy], feed_dict=feed_dict)\n",
    "        avg_cost += c\n",
    "        avg_acc += a\n",
    "        \n",
    "    avg_cost = avg_cost / (len(x_train) / batch_size)    \n",
    "    avg_acc = avg_acc / (len(x_train) / batch_size)    \n",
    "    print(\"epoch: {} cost: {:.5f} acc: {:.5f}\".format(epoch, avg_cost, avg_acc))\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submission(predictions, test_id):\n",
    "    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n",
    "    now = datetime.datetime.now()\n",
    "    suffix = str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "    sub_file = os.path.join('submission_' + suffix + '.csv')\n",
    "    result1.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_arr = []\n",
    "for step in range(0, len(x_test), batch_size):\n",
    "    feed_dict = {X: x_test[step:step+batch_size], dropout: 1}\n",
    "\n",
    "    preds = sess.run(pred, feed_dict=feed_dict)\n",
    "    \n",
    "    for tmp in preds:\n",
    "        pred_arr.append(tmp)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_submission(pred_arr, test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future work/exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Multiple Initialization Techniques  \n",
    "* Study of image size  \n",
    "* batch normalization\n",
    "* data augmentation\n",
    "* cross-validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
